{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1RjlGKol54ECZqe1tWIYpQuuu9t-835SH",
      "authorship_tag": "ABX9TyPDzNVQ69BCd5U4aLorqg9M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bala1802/ERA/blob/main/Part-2/S6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-xDmm6vN_CWB"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding=1),\n",
        "                        nn.BatchNorm2d(4),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.conv2 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, padding=1),\n",
        "                        nn.BatchNorm2d(16),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.conv3 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "                        nn.BatchNorm2d(32),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.conv4 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU(inplace=True))\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Sequential(\n",
        "                        nn.Dropout(p=0.2),\n",
        "                        nn.Linear(in_features=64, out_features=10))\n",
        "       \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        if x.dim() == 2:\n",
        "          x = x.unsqueeze(2).unsqueeze(3)\n",
        "        elif x.dim() == 3:\n",
        "            x = x.unsqueeze(0)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view((x.shape[0],-1))\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2JodrdUVeoBE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Number of parameters in the model: {}\".format(num_params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IirZmJTYj5YF",
        "outputId": "06b0238b-d176-41bc-c4f0-acc6d434e8d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters in the model: 24650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjsgN0GM_fYG",
        "outputId": "e385f135-a6dd-4475-eab1-2d5dd5876ab2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 4, 28, 28]              40\n",
            "       BatchNorm2d-2            [-1, 4, 28, 28]               8\n",
            "              ReLU-3            [-1, 4, 28, 28]               0\n",
            "         MaxPool2d-4            [-1, 4, 14, 14]               0\n",
            "            Conv2d-5           [-1, 16, 14, 14]             592\n",
            "       BatchNorm2d-6           [-1, 16, 14, 14]              32\n",
            "              ReLU-7           [-1, 16, 14, 14]               0\n",
            "         MaxPool2d-8             [-1, 16, 7, 7]               0\n",
            "            Conv2d-9             [-1, 32, 7, 7]           4,640\n",
            "      BatchNorm2d-10             [-1, 32, 7, 7]              64\n",
            "             ReLU-11             [-1, 32, 7, 7]               0\n",
            "        MaxPool2d-12             [-1, 32, 3, 3]               0\n",
            "           Conv2d-13             [-1, 64, 3, 3]          18,496\n",
            "      BatchNorm2d-14             [-1, 64, 3, 3]             128\n",
            "             ReLU-15             [-1, 64, 3, 3]               0\n",
            "AdaptiveAvgPool2d-16             [-1, 64, 1, 1]               0\n",
            "          Dropout-17                   [-1, 64]               0\n",
            "           Linear-18                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 24,650\n",
            "Trainable params: 24,650\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.21\n",
            "Params size (MB): 0.09\n",
            "Estimated Total Size (MB): 0.30\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "metadata": {
        "id": "6FfSLq-V_hvc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST('../data', train=True, download=True,  \n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))\n",
        "test_dataset = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))"
      ],
      "metadata": {
        "id": "6Wv3GoPS_uQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "466d7987-7be0-4d17-956e-d456b8215bdc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 148061649.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 102919026.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 34754096.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 26606883.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide the train_dataset into `train_dataset` and `validation_dataset`\n",
        "train_dataset_size = int(0.8 * len(train_dataset))\n",
        "validation_dataset_size = len(train_dataset) - train_dataset_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_dataset_size, validation_dataset_size])"
      ],
      "metadata": {
        "id": "dTjsvfljCdCp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "validation_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size, shuffle=True, **kwargs)"
      ],
      "metadata": {
        "id": "ejmJzLbDCf6N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(validation_loader), len(test_loader)"
      ],
      "metadata": {
        "id": "-LFsgFDICozP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd44871-0e92-495d-9735-47566fcac0c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(375, 94, 79)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "metadata": {
        "id": "q0VO1uxqDISO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4273fbb-93c3-43b5-c6d9-d9d38cc9252f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 12000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "# Some initialization work first...\n",
        "epochs = 19\n",
        "train_losses, val_losses = [], []\n",
        "train_accu, val_accu = [], []\n",
        "start_time = time.time()\n",
        "early_stop_counter = 10   # stop when the validation loss does not improve for 10 iterations to prevent overfitting\n",
        "counter = 0\n",
        "best_val_loss = float('Inf')"
      ],
      "metadata": {
        "id": "Q0FEE8R2JgFT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model.to(device)\n",
        "criterion = nn.NLLLoss()   # with log_softmax() as the last layer, this is equivalent to cross entropy loss\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "uxLcbYk1Ql-E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(epochs):\n",
        "  \n",
        "  epoch_start_time = time.time()\n",
        "  running_loss = 0\n",
        "  accuracy=0\n",
        "  # training step\n",
        "  model.train()\n",
        "\n",
        "  for batch_index, (images, labels) in enumerate(train_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    log_ps = model(images)\n",
        "\n",
        "    ps = torch.exp(log_ps)\n",
        "    top_p, top_class = ps.topk(1, dim=1)\n",
        "    equals = top_class == labels.view(*top_class.shape)\n",
        "\n",
        "    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    loss = criterion(log_ps, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  train_losses.append(running_loss/len(train_loader))\n",
        "  train_accu.append(accuracy/len(train_loader))\n",
        "\n",
        "# Validation\n",
        "  val_loss = 0\n",
        "  accuracy=0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_index, (images, labels) in enumerate(validation_loader):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      log_ps = model(images)\n",
        "      val_loss += criterion(log_ps, labels)\n",
        "\n",
        "      ps = torch.exp(log_ps)\n",
        "      top_p, top_class = ps.topk(1, dim=1)\n",
        "      equals = top_class == labels.view(*top_class.shape)\n",
        "      accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "    val_losses.append(val_loss/len(validation_loader))\n",
        "    val_accu.append(accuracy/len(validation_loader))\n",
        "\n",
        "    print(\"Epoch: {}/{}.. \".format(e+1, epochs), \"Time: {:.2f}s..\".format(time.time()-epoch_start_time), \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "          \"Training Accu: {:.3f}.. \".format(train_accu[-1]), \"Val Loss: {:.3f}.. \".format(val_losses[-1]), \"Val Accu: {:.3f}\".format(val_accu[-1]))\n",
        "    \n",
        "    if val_losses[-1] < best_val_loss:\n",
        "        best_val_loss = val_losses[-1]\n",
        "        counter=0\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    else:\n",
        "        counter+=1\n",
        "        print('Validation loss has not improved since: {:.3f}..'.format(best_val_loss), 'Count: ', str(counter))\n",
        "        if counter >= early_stop_counter:\n",
        "            print('Early Stopping Now!!!!')\n",
        "            model.load_state_dict(best_model_wts)\n",
        "            break"
      ],
      "metadata": {
        "id": "qML8hiWfQBp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46184d53-422f-45d4-e5f3-04017ed05c6d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/19..  Time: 17.44s.. Training Loss: 0.333..  Training Accu: 0.893..  Val Loss: 0.085..  Val Accu: 0.971\n",
            "Epoch: 2/19..  Time: 17.43s.. Training Loss: 0.090..  Training Accu: 0.973..  Val Loss: 0.108..  Val Accu: 0.969\n",
            "Validation loss has not improved since: 0.085.. Count:  1\n",
            "Epoch: 3/19..  Time: 16.70s.. Training Loss: 0.074..  Training Accu: 0.978..  Val Loss: 0.070..  Val Accu: 0.980\n",
            "Epoch: 4/19..  Time: 16.71s.. Training Loss: 0.065..  Training Accu: 0.981..  Val Loss: 0.078..  Val Accu: 0.977\n",
            "Validation loss has not improved since: 0.070.. Count:  1\n",
            "Epoch: 5/19..  Time: 16.68s.. Training Loss: 0.064..  Training Accu: 0.981..  Val Loss: 0.123..  Val Accu: 0.967\n",
            "Validation loss has not improved since: 0.070.. Count:  2\n",
            "Epoch: 6/19..  Time: 17.79s.. Training Loss: 0.058..  Training Accu: 0.983..  Val Loss: 0.055..  Val Accu: 0.983\n",
            "Epoch: 7/19..  Time: 16.76s.. Training Loss: 0.055..  Training Accu: 0.983..  Val Loss: 0.094..  Val Accu: 0.975\n",
            "Validation loss has not improved since: 0.055.. Count:  1\n",
            "Epoch: 8/19..  Time: 16.89s.. Training Loss: 0.054..  Training Accu: 0.984..  Val Loss: 0.064..  Val Accu: 0.983\n",
            "Validation loss has not improved since: 0.055.. Count:  2\n",
            "Epoch: 9/19..  Time: 17.04s.. Training Loss: 0.051..  Training Accu: 0.985..  Val Loss: 0.044..  Val Accu: 0.988\n",
            "Epoch: 10/19..  Time: 17.87s.. Training Loss: 0.048..  Training Accu: 0.986..  Val Loss: 0.049..  Val Accu: 0.986\n",
            "Validation loss has not improved since: 0.044.. Count:  1\n",
            "Epoch: 11/19..  Time: 17.14s.. Training Loss: 0.048..  Training Accu: 0.986..  Val Loss: 0.054..  Val Accu: 0.985\n",
            "Validation loss has not improved since: 0.044.. Count:  2\n",
            "Epoch: 12/19..  Time: 16.84s.. Training Loss: 0.044..  Training Accu: 0.987..  Val Loss: 0.067..  Val Accu: 0.982\n",
            "Validation loss has not improved since: 0.044.. Count:  3\n",
            "Epoch: 13/19..  Time: 16.80s.. Training Loss: 0.042..  Training Accu: 0.987..  Val Loss: 0.060..  Val Accu: 0.983\n",
            "Validation loss has not improved since: 0.044.. Count:  4\n",
            "Epoch: 14/19..  Time: 17.68s.. Training Loss: 0.048..  Training Accu: 0.986..  Val Loss: 0.053..  Val Accu: 0.985\n",
            "Validation loss has not improved since: 0.044.. Count:  5\n",
            "Epoch: 15/19..  Time: 16.80s.. Training Loss: 0.045..  Training Accu: 0.987..  Val Loss: 0.057..  Val Accu: 0.985\n",
            "Validation loss has not improved since: 0.044.. Count:  6\n",
            "Epoch: 16/19..  Time: 16.60s.. Training Loss: 0.045..  Training Accu: 0.987..  Val Loss: 0.047..  Val Accu: 0.987\n",
            "Validation loss has not improved since: 0.044.. Count:  7\n",
            "Epoch: 17/19..  Time: 16.81s.. Training Loss: 0.039..  Training Accu: 0.988..  Val Loss: 0.052..  Val Accu: 0.988\n",
            "Validation loss has not improved since: 0.044.. Count:  8\n",
            "Epoch: 18/19..  Time: 17.65s.. Training Loss: 0.040..  Training Accu: 0.988..  Val Loss: 0.046..  Val Accu: 0.988\n",
            "Validation loss has not improved since: 0.044.. Count:  9\n",
            "Epoch: 19/19..  Time: 16.64s.. Training Loss: 0.037..  Training Accu: 0.989..  Val Loss: 0.045..  Val Accu: 0.988\n",
            "Validation loss has not improved since: 0.044.. Count:  10\n",
            "Early Stopping Now!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "moOwXP8fbBlt"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}