{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1RjlGKol54ECZqe1tWIYpQuuu9t-835SH",
      "authorship_tag": "ABX9TyNm6A0b3lGCDm/v4mhglKnz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bala1802/ERA/blob/main/Session-6/Part-2/S6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-xDmm6vN_CWB"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
        "                        nn.BatchNorm2d(16),\n",
        "                        nn.ReLU(inplace=True))\n",
        "        self.conv2 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "                        nn.BatchNorm2d(32),\n",
        "                        nn.ReLU(inplace=True))\n",
        "        self.conv3 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Sequential(\n",
        "                        nn.Dropout(p=0.25),\n",
        "                        nn.Linear(in_features=64, out_features=10))\n",
        "       \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        if x.dim() == 2:\n",
        "          x = x.unsqueeze(2).unsqueeze(3)\n",
        "        elif x.dim() == 3:\n",
        "            x = x.unsqueeze(0)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view((x.shape[0],-1))\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2JodrdUVeoBE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Number of parameters in the model: {}\".format(num_params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IirZmJTYj5YF",
        "outputId": "f50fc205-f387-4dce-bddc-f12a2536e5ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters in the model: 24170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjsgN0GM_fYG",
        "outputId": "3f6ec83e-a425-4f32-a07e-0677167890f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             160\n",
            "       BatchNorm2d-2           [-1, 16, 28, 28]              32\n",
            "              ReLU-3           [-1, 16, 28, 28]               0\n",
            "            Conv2d-4           [-1, 32, 28, 28]           4,640\n",
            "       BatchNorm2d-5           [-1, 32, 28, 28]              64\n",
            "              ReLU-6           [-1, 32, 28, 28]               0\n",
            "            Conv2d-7           [-1, 64, 28, 28]          18,496\n",
            "       BatchNorm2d-8           [-1, 64, 28, 28]             128\n",
            "              ReLU-9           [-1, 64, 28, 28]               0\n",
            "        MaxPool2d-10           [-1, 64, 14, 14]               0\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "          Dropout-12                   [-1, 64]               0\n",
            "           Linear-13                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 24,170\n",
            "Trainable params: 24,170\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 2.11\n",
            "Params size (MB): 0.09\n",
            "Estimated Total Size (MB): 2.20\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "metadata": {
        "id": "6FfSLq-V_hvc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST('../data', train=True, download=True,  \n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))\n",
        "test_dataset = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))"
      ],
      "metadata": {
        "id": "6Wv3GoPS_uQa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide the train_dataset into `train_dataset` and `validation_dataset`\n",
        "train_dataset_size = int(0.8 * len(train_dataset))\n",
        "validation_dataset_size = len(train_dataset) - train_dataset_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_dataset_size, validation_dataset_size])"
      ],
      "metadata": {
        "id": "dTjsvfljCdCp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "validation_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size, shuffle=True, **kwargs)"
      ],
      "metadata": {
        "id": "ejmJzLbDCf6N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(validation_loader), len(test_loader)"
      ],
      "metadata": {
        "id": "-LFsgFDICozP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5096fd50-fa17-4634-c482-b3fee3e91e9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(375, 94, 79)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "metadata": {
        "id": "q0VO1uxqDISO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05986423-877b-46d4-f81a-aa2de1450d7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 12000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "# Some initialization work first...\n",
        "epochs = 19\n",
        "train_losses, val_losses = [], []\n",
        "train_accu, val_accu = [], []\n",
        "start_time = time.time()\n",
        "early_stop_counter = 10   # stop when the validation loss does not improve for 10 iterations to prevent overfitting\n",
        "counter = 0\n",
        "best_val_loss = float('Inf')"
      ],
      "metadata": {
        "id": "Q0FEE8R2JgFT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model.to(device)\n",
        "criterion = nn.NLLLoss()   # with log_softmax() as the last layer, this is equivalent to cross entropy loss\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "uxLcbYk1Ql-E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(epochs):\n",
        "  \n",
        "  epoch_start_time = time.time()\n",
        "  running_loss = 0\n",
        "  accuracy=0\n",
        "  # training step\n",
        "  model.train()\n",
        "\n",
        "  for batch_index, (images, labels) in enumerate(train_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    log_ps = model(images)\n",
        "\n",
        "    ps = torch.exp(log_ps)\n",
        "    top_p, top_class = ps.topk(1, dim=1)\n",
        "    equals = top_class == labels.view(*top_class.shape)\n",
        "\n",
        "    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    loss = criterion(log_ps, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  train_losses.append(running_loss/len(train_loader))\n",
        "  train_accu.append(accuracy/len(train_loader))\n",
        "\n",
        "# Validation\n",
        "  val_loss = 0\n",
        "  accuracy=0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_index, (images, labels) in enumerate(validation_loader):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      log_ps = model(images)\n",
        "      val_loss += criterion(log_ps, labels)\n",
        "\n",
        "      ps = torch.exp(log_ps)\n",
        "      top_p, top_class = ps.topk(1, dim=1)\n",
        "      equals = top_class == labels.view(*top_class.shape)\n",
        "      accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "    val_losses.append(val_loss/len(validation_loader))\n",
        "    val_accu.append(accuracy/len(validation_loader))\n",
        "\n",
        "    print(\"Epoch: {}/{}.. \".format(e+1, epochs), \"Time: {:.2f}s..\".format(time.time()-epoch_start_time), \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "          \"Training Accu: {:.3f}.. \".format(train_accu[-1]), \"Val Loss: {:.3f}.. \".format(val_losses[-1]), \"Val Accu: {:.3f}\".format(val_accu[-1]))\n",
        "    \n",
        "    if val_losses[-1] < best_val_loss:\n",
        "        best_val_loss = val_losses[-1]\n",
        "        counter=0\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    else:\n",
        "        counter+=1\n",
        "        print('Validation loss has not improved since: {:.3f}..'.format(best_val_loss), 'Count: ', str(counter))\n",
        "        if counter >= early_stop_counter:\n",
        "            print('Early Stopping Now!!!!')\n",
        "            model.load_state_dict(best_model_wts)\n",
        "            break"
      ],
      "metadata": {
        "id": "qML8hiWfQBp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb57559-4f09-43bf-c93f-5d0722a85e1a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/19..  Time: 24.88s.. Training Loss: 1.496..  Training Accu: 0.447..  Val Loss: 1.026..  Val Accu: 0.608\n",
            "Epoch: 2/19..  Time: 18.49s.. Training Loss: 0.782..  Training Accu: 0.733..  Val Loss: 0.460..  Val Accu: 0.860\n",
            "Epoch: 3/19..  Time: 18.06s.. Training Loss: 0.578..  Training Accu: 0.810..  Val Loss: 0.335..  Val Accu: 0.895\n",
            "Epoch: 4/19..  Time: 19.84s.. Training Loss: 0.481..  Training Accu: 0.843..  Val Loss: 0.246..  Val Accu: 0.934\n",
            "Epoch: 5/19..  Time: 18.02s.. Training Loss: 0.430..  Training Accu: 0.861..  Val Loss: 0.221..  Val Accu: 0.940\n",
            "Epoch: 6/19..  Time: 18.62s.. Training Loss: 0.403..  Training Accu: 0.873..  Val Loss: 0.209..  Val Accu: 0.937\n",
            "Epoch: 7/19..  Time: 19.00s.. Training Loss: 0.380..  Training Accu: 0.878..  Val Loss: 0.206..  Val Accu: 0.943\n",
            "Epoch: 8/19..  Time: 17.98s.. Training Loss: 0.358..  Training Accu: 0.885..  Val Loss: 0.185..  Val Accu: 0.950\n",
            "Epoch: 9/19..  Time: 18.11s.. Training Loss: 0.345..  Training Accu: 0.890..  Val Loss: 0.230..  Val Accu: 0.930\n",
            "Validation loss has not improved since: 0.185.. Count:  1\n",
            "Epoch: 10/19..  Time: 19.18s.. Training Loss: 0.346..  Training Accu: 0.889..  Val Loss: 0.176..  Val Accu: 0.948\n",
            "Epoch: 11/19..  Time: 18.08s.. Training Loss: 0.335..  Training Accu: 0.893..  Val Loss: 0.165..  Val Accu: 0.954\n",
            "Epoch: 12/19..  Time: 18.12s.. Training Loss: 0.329..  Training Accu: 0.895..  Val Loss: 0.166..  Val Accu: 0.950\n",
            "Validation loss has not improved since: 0.165.. Count:  1\n",
            "Epoch: 13/19..  Time: 19.10s.. Training Loss: 0.316..  Training Accu: 0.899..  Val Loss: 0.162..  Val Accu: 0.958\n",
            "Epoch: 14/19..  Time: 18.01s.. Training Loss: 0.321..  Training Accu: 0.897..  Val Loss: 0.142..  Val Accu: 0.961\n",
            "Epoch: 15/19..  Time: 18.35s.. Training Loss: 0.309..  Training Accu: 0.901..  Val Loss: 0.165..  Val Accu: 0.952\n",
            "Validation loss has not improved since: 0.142.. Count:  1\n",
            "Epoch: 16/19..  Time: 19.16s.. Training Loss: 0.305..  Training Accu: 0.903..  Val Loss: 0.140..  Val Accu: 0.959\n",
            "Epoch: 17/19..  Time: 17.94s.. Training Loss: 0.302..  Training Accu: 0.902..  Val Loss: 0.143..  Val Accu: 0.960\n",
            "Validation loss has not improved since: 0.140.. Count:  1\n",
            "Epoch: 18/19..  Time: 18.31s.. Training Loss: 0.291..  Training Accu: 0.907..  Val Loss: 0.154..  Val Accu: 0.957\n",
            "Validation loss has not improved since: 0.140.. Count:  2\n",
            "Epoch: 19/19..  Time: 19.09s.. Training Loss: 0.290..  Training Accu: 0.908..  Val Loss: 0.151..  Val Accu: 0.955\n",
            "Validation loss has not improved since: 0.140.. Count:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "moOwXP8fbBlt"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}